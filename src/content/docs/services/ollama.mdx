---
title: Ollama
head:
  - tag: "meta"
    attrs:
        property: "og:title"
        content: "How to host Ollama with Coolify"
description: "Here you can find the documentation for hosting Ollama with Coolify."
---

import { Badge } from '@astrojs/starlight/components';

<Badge text="One-click setup." variant="note" size="large" />

![Ollama](../../../assets/images/services/ollama.png)

## What is Ollama?

Ollama is a lightweight and efficient server for running large language models (LLMs) on your local machine or in the cloud.

It includes OpenWebUI, a web-based interface for interacting with the models.

{/* ## Screenshots */}

{/* ![Ollama](../../../assets/images/services/ollama.gif) */}

## Links

- [The official website ›](https://ollama.com/?utm_source=coolify.io)
- [GitHub ›](https://github.com/ollama/ollama?utm_source=coolify.io)
